{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24158c6",
   "metadata": {},
   "source": [
    "# RNN Text Classification with Multiple Embeddings\n",
    "This notebook trains a SimpleRNN model using three embedding techniques: TF-IDF, Word2Vec Skip-gram, and Word2Vec CBOW.\n",
    "\n",
    "It follows a shared preprocessing pipeline and reports comparable metrics for each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef80400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Keras / TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117132e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = os.path.join(\"..\", \"data\", \"Reviews.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select text and label columns with a robust fallback\n",
    "def pick_columns(dataframe):\n",
    "    text_col_candidates = [\"Text\", \"text\", \"review\", \"review_text\", \"sentence\"]\n",
    "    label_col_candidates = [\"Score\", \"label\", \"sentiment\", \"Sentiment\", \"target\"]\n",
    "\n",
    "    text_col = next((c for c in text_col_candidates if c in dataframe.columns), None)\n",
    "    label_col = next((c for c in label_col_candidates if c in dataframe.columns), None)\n",
    "\n",
    "    if text_col is None:\n",
    "        raise ValueError(\"No text column found. Update text_col_candidates.\")\n",
    "    if label_col is None:\n",
    "        raise ValueError(\"No label column found. Update label_col_candidates.\")\n",
    "    return text_col, label_col\n",
    "\n",
    "text_col, label_col = pick_columns(df)\n",
    "print(\"Using text column:\", text_col)\n",
    "print(\"Using label column:\", label_col)\n",
    "\n",
    "data = df[[text_col, label_col]].dropna().copy()\n",
    "\n",
    "# Binarize if label column is a review score\n",
    "if label_col.lower() == \"score\":\n",
    "    data = data[data[label_col] != 3]\n",
    "    data[\"label\"] = (data[label_col] >= 4).astype(int)\n",
    "else:\n",
    "    # Assume labels are already binary or can be mapped\n",
    "    unique_labels = data[label_col].unique()\n",
    "    if set(unique_labels) <= {0, 1}:\n",
    "        data[\"label\"] = data[label_col].astype(int)\n",
    "    else:\n",
    "        # Simple mapping for common sentiment labels\n",
    "        label_map = {\"negative\": 0, \"positive\": 1, \"neg\": 0, \"pos\": 1}\n",
    "        data[\"label\"] = data[label_col].astype(str).str.lower().map(label_map)\n",
    "        data = data.dropna(subset=[\"label\"]).copy()\n",
    "        data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "data.rename(columns={text_col: \"text\"}, inplace=True)\n",
    "data = data[[\"text\", \"label\"]]\n",
    "print(data.head())\n",
    "print(data[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca69c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "data[\"clean_text\"] = data[\"text\"].apply(clean_text)\n",
    "data[\"text_len\"] = data[\"clean_text\"].str.split().apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: class balance and text length\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "data[\"label\"].value_counts().plot(kind=\"bar\", ax=axes[0], title=\"Class Distribution\")\n",
    "data[\"text_len\"].plot(kind=\"hist\", bins=50, ax=axes[1], title=\"Text Length (tokens)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "X = data[\"clean_text\"].values\n",
    "y = data[\"label\"].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    " )\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
    " )\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared utilities\n",
    "def evaluate_binary(y_true, y_pred, label=\"model\"):\n",
    "    y_hat = (y_pred >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    f1 = f1_score(y_true, y_hat)\n",
    "    print(f\"{label} Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_hat))\n",
    "    cm = confusion_matrix(y_true, y_hat)\n",
    "    return acc, f1, cm\n",
    "\n",
    "def build_rnn(input_shape, rnn_units=64, dropout=0.2):\n",
    "    model = Sequential([\n",
    "        SimpleRNN(rnn_units, input_shape=input_shape),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_with_early_stop(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=128):\n",
    "    callback = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[callback],\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with TF-IDF features\n",
    "MAX_FEATURES = 2000\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Convert to dense and reshape as a pseudo-sequence: (samples, timesteps, 1)\n",
    "X_train_tfidf_rnn = X_train_tfidf.toarray().reshape(-1, MAX_FEATURES, 1)\n",
    "X_val_tfidf_rnn = X_val_tfidf.toarray().reshape(-1, MAX_FEATURES, 1)\n",
    "X_test_tfidf_rnn = X_test_tfidf.toarray().reshape(-1, MAX_FEATURES, 1)\n",
    "\n",
    "tfidf_rnn = build_rnn(input_shape=(MAX_FEATURES, 1), rnn_units=64)\n",
    "_ = train_with_early_stop(tfidf_rnn, X_train_tfidf_rnn, y_train, X_val_tfidf_rnn, y_val, epochs=8)\n",
    "\n",
    "tfidf_pred = tfidf_rnn.predict(X_test_tfidf_rnn).ravel()\n",
    "tfidf_acc, tfidf_f1, tfidf_cm = evaluate_binary(y_test, tfidf_pred, label=\"TF-IDF RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303839d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with Word2Vec embeddings (Skip-gram and CBOW)\n",
    "MAX_VOCAB = 20000\n",
    "MAX_LEN = 200\n",
    "EMBED_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(MAX_VOCAB, len(word_index) + 1)\n",
    "\n",
    "def train_word2vec(sentences, sg_mode=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=EMBED_DIM,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=sg_mode,\n",
    "        seed=SEED,\n",
    "        epochs=10,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_embedding_matrix(w2v_model, word_index, vocab_size, embed_dim):\n",
    "    matrix = np.zeros((vocab_size, embed_dim), dtype=np.float32)\n",
    "    for word, idx in word_index.items():\n",
    "        if idx >= vocab_size:\n",
    "            continue\n",
    "        if word in w2v_model.wv:\n",
    "            matrix[idx] = w2v_model.wv[word]\n",
    "    return matrix\n",
    "\n",
    "def build_rnn_with_embedding(embedding_matrix, max_len):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False,\n",
    "        ),\n",
    "        SimpleRNN(64),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Prepare tokenized sentences for Word2Vec training\n",
    "train_tokens = [text.split() for text in X_train]\n",
    "\n",
    "# Skip-gram (sg=1)\n",
    "w2v_skip = train_word2vec(train_tokens, sg_mode=1)\n",
    "skip_matrix = build_embedding_matrix(w2v_skip, word_index, vocab_size, EMBED_DIM)\n",
    "skip_rnn = build_rnn_with_embedding(skip_matrix, MAX_LEN)\n",
    "_ = train_with_early_stop(skip_rnn, X_train_pad, y_train, X_val_pad, y_val, epochs=8)\n",
    "skip_pred = skip_rnn.predict(X_test_pad).ravel()\n",
    "skip_acc, skip_f1, skip_cm = evaluate_binary(y_test, skip_pred, label=\"Word2Vec Skip-gram RNN\")\n",
    "\n",
    "# CBOW (sg=0)\n",
    "w2v_cbow = train_word2vec(train_tokens, sg_mode=0)\n",
    "cbow_matrix = build_embedding_matrix(w2v_cbow, word_index, vocab_size, EMBED_DIM)\n",
    "cbow_rnn = build_rnn_with_embedding(cbow_matrix, MAX_LEN)\n",
    "_ = train_with_early_stop(cbow_rnn, X_train_pad, y_train, X_val_pad, y_val, epochs=8)\n",
    "cbow_pred = cbow_rnn.predict(X_test_pad).ravel()\n",
    "cbow_acc, cbow_f1, cbow_cm = evaluate_binary(y_test, cbow_pred, label=\"Word2Vec CBOW RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "results = pd.DataFrame(\n",
    "    [\n",
    "        {\"Embedding\": \"TF-IDF\", \"Accuracy\": tfidf_acc, \"F1\": tfidf_f1},\n",
    "        {\"Embedding\": \"Word2Vec Skip-gram\", \"Accuracy\": skip_acc, \"F1\": skip_f1},\n",
    "        {\"Embedding\": \"Word2Vec CBOW\", \"Accuracy\": cbow_acc, \"F1\": cbow_f1},\n",
    "    ]\n",
    ").sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
