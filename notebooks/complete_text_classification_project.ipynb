{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce41c319",
   "metadata": {},
   "source": [
    "# Text Classification with Multiple Word Embeddings\n",
    "\n",
    "\n",
    "\n",
    "**Dataset**: Amazon Fine Food Reviews (1-5 star ratings)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5d1a2",
   "metadata": {},
   "source": [
    "# 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281abad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Connection timed out while downloading.\n",
      "WARNING: Attempting to resume incomplete download (5.0 MB/36.3 MB, attempt 1)\n",
      "WARNING: Connection timed out while downloading.\n",
      "WARNING: Attempting to resume incomplete download (12.3 MB/36.3 MB, attempt 2)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 100, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ~~~~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 96, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 419, in run\n",
      "    preparer.prepare_linked_requirements_more(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        requirement_set.requirements.values()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 569, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 478, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 183, in batch\n",
      "    filepath, content_type = self(link, location)\n",
      "                             ~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 196, in __call__\n",
      "    self._attempt_resumes_or_redownloads(download, resp)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 246, in _attempt_resumes_or_redownloads\n",
      "    self._process_response(download, resume_resp)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 211, in _process_response\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\Cloned repos\\text_classification\\text_classification_with_multiple_embeddings\\.venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy matplotlib seaborn wordcloud nltk scikit-learn tensorflow gensim beautifulsoup4 -q\n",
    "\n",
    "print(\"✅ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12b41a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mssl\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Handle SSL certificate issues (if any)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# Handle SSL certificate issues (if any)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print(\"NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c056bee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import all required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85b312",
   "metadata": {},
   "source": [
    "# 2. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'DATA_PATH': '../Reviews.csv',\n",
    "    'SAMPLE_SIZE': 50000,  # Use None for full dataset, or integer for testing\n",
    "    'TEST_SIZE': 0.2,\n",
    "    'VAL_SIZE': 0.1,\n",
    "    'RANDOM_STATE': 42,\n",
    "    \n",
    "    # Text preprocessing\n",
    "    'MAX_SEQUENCE_LENGTH': 200,\n",
    "    'MIN_WORD_FREQ': 2,\n",
    "    'MAX_VOCAB_SIZE': 50000,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'NUM_CLASSES': 5,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 30,  # Reduced for faster training in notebook\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    \n",
    "    # GRU architecture\n",
    "    'GRU_UNITS': 128,\n",
    "    'DROPOUT_RATE': 0.5,\n",
    "    'RECURRENT_DROPOUT': 0.2,\n",
    "    'USE_BIDIRECTIONAL': True,\n",
    "    'NUM_GRU_LAYERS': 2,\n",
    "    \n",
    "    # Embedding dimensions\n",
    "    'EMBEDDING_DIM': 100,\n",
    "    'TFIDF_MAX_FEATURES': 5000,\n",
    "    \n",
    "    # Training\n",
    "    'EARLY_STOPPING_PATIENCE': 5,\n",
    "    'REDUCE_LR_PATIENCE': 3,\n",
    "    'USE_CLASS_WEIGHTS': True,\n",
    "    \n",
    "    # Class names\n",
    "    'CLASS_NAMES': ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdae14",
   "metadata": {},
   "source": [
    "# 3. Helper Functions\n",
    "\n",
    "We'll define all helper functions here that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Class\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, remove_stopwords=True, lemmatize=True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "        self.lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags using BeautifulSoup.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text, return_string=False):\n",
    "        \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "        # Remove HTML\n",
    "        text = self.remove_html_tags(text)\n",
    "        # Clean\n",
    "        text = self.clean_text(text)\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        # Lemmatize\n",
    "        if self.lemmatize and self.lemmatizer:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        # Filter short tokens\n",
    "        tokens = [t for t in tokens if len(t) >= 2]\n",
    "        \n",
    "        return ' '.join(tokens) if return_string else tokens\n",
    "\n",
    "print(\"✅ TextPreprocessor class defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
