{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce41c319",
   "metadata": {},
   "source": [
    "# Text Classification with Multiple Word Embeddings\n",
    "\n",
    "\n",
    "\n",
    "**Dataset**: Amazon Fine Food Reviews (1-5 star ratings)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5d1a2",
   "metadata": {},
   "source": [
    "# 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281abad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy matplotlib seaborn wordcloud nltk scikit-learn tensorflow gensim beautifulsoup4 -q\n",
    "\n",
    "print(\"✅ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data (run once)\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"✅ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85b312",
   "metadata": {},
   "source": [
    "# 2. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'DATA_PATH': '../Reviews.csv',\n",
    "    'SAMPLE_SIZE': 50000,  # Use None for full dataset, or integer for testing\n",
    "    'TEST_SIZE': 0.2,\n",
    "    'VAL_SIZE': 0.1,\n",
    "    'RANDOM_STATE': 42,\n",
    "    \n",
    "    # Text preprocessing\n",
    "    'MAX_SEQUENCE_LENGTH': 200,\n",
    "    'MIN_WORD_FREQ': 2,\n",
    "    'MAX_VOCAB_SIZE': 50000,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'NUM_CLASSES': 5,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 30,  # Reduced for faster training in notebook\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    \n",
    "    # GRU architecture\n",
    "    'GRU_UNITS': 128,\n",
    "    'DROPOUT_RATE': 0.5,\n",
    "    'RECURRENT_DROPOUT': 0.2,\n",
    "    'USE_BIDIRECTIONAL': True,\n",
    "    'NUM_GRU_LAYERS': 2,\n",
    "    \n",
    "    # Embedding dimensions\n",
    "    'EMBEDDING_DIM': 100,\n",
    "    'TFIDF_MAX_FEATURES': 5000,\n",
    "    \n",
    "    # Training\n",
    "    'EARLY_STOPPING_PATIENCE': 5,\n",
    "    'REDUCE_LR_PATIENCE': 3,\n",
    "    'USE_CLASS_WEIGHTS': True,\n",
    "    \n",
    "    # Class names\n",
    "    'CLASS_NAMES': ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdae14",
   "metadata": {},
   "source": [
    "# 3. Helper Functions\n",
    "\n",
    "We'll define all helper functions here that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Class\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, remove_stopwords=True, lemmatize=True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "        self.lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags using BeautifulSoup.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text, return_string=False):\n",
    "        \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "        # Remove HTML\n",
    "        text = self.remove_html_tags(text)\n",
    "        # Clean\n",
    "        text = self.clean_text(text)\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        # Lemmatize\n",
    "        if self.lemmatize and self.lemmatizer:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        # Filter short tokens\n",
    "        tokens = [t for t in tokens if len(t) >= 2]\n",
    "        \n",
    "        return ' '.join(tokens) if return_string else tokens\n",
    "\n",
    "print(\"✅ TextPreprocessor class defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
